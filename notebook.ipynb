{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e7663280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Stuff Documents Chain RAG pipeline with memory ===\n",
      "\n",
      "1. Aaronson이 유죄인가?\n",
      "Q: Aaronson은 유죄인가요?\n",
      "A: 네, Jones, Aaronson 및 Rutherford는 그들이 기소된 범죄에 유죄로 판결받았습니다. 그러나 그들의 유죄를 반증하는 사진을 본 적이 없다고 기억합니다. 사실, 그런 사진은 존재하지 않았고, 유죄 판결을 뒷받침하는 증거를 그가 발명했다고 기억합니다.\n",
      "--------------------------------------------------\n",
      "2. 그가 테이블에 어떤 메시지를 썼나요?\n",
      "Q: 그가 테이블에 어떤 메시지를 썼나요?\n",
      "A: 그가 테이블에 썼던 메시지는 \"2수 안에 백이 승리합니다\"였습니다. 이것은 체스 문제를 의미하는 것으로, 그가 백색 말을 움직여서 2수 안에 상대를 승리시킬 수 있다는 것을 나타냅니다.\n",
      "--------------------------------------------------\n",
      "3. Julia는 누구인가요?\n",
      "Q: Julia는 누구인가요?\n",
      "A: Julia는 주인공인 윈스턴과 사랑을 나누는 여성 캐릭터입니다. 그들은 미래의 오세아니아에서 파티의 감시와 통제 속에서 금지된 사랑을 경험하게 됩니다. 그러나 이 사랑은 파티의 규칙을 어기는 것으로 간주되며 위험한 상황에 처하게 됩니다.\n",
      "--------------------------------------------------\n",
      "=== memory contents ===\n",
      "{'chat_history': [HumanMessage(content='Aaronson은 유죄인가요?'), AIMessage(content='네, Jones, Aaronson 및 Rutherford는 그들이 기소된 범죄에 유죄로 판결받았습니다. 그러나 그들의 유죄를 반증하는 사진을 본 적이 없다고 기억합니다. 사실, 그런 사진은 존재하지 않았고, 유죄 판결을 뒷받침하는 증거를 그가 발명했다고 기억합니다.'), HumanMessage(content='그가 테이블에 어떤 메시지를 썼나요?'), AIMessage(content='그가 테이블에 썼던 메시지는 \"2수 안에 백이 승리합니다\"였습니다. 이것은 체스 문제를 의미하는 것으로, 그가 백색 말을 움직여서 2수 안에 상대를 승리시킬 수 있다는 것을 나타냅니다.'), HumanMessage(content='Julia는 누구인가요?'), AIMessage(content='Julia는 주인공인 윈스턴과 사랑을 나누는 여성 캐릭터입니다. 그들은 미래의 오세아니아에서 파티의 감시와 통제 속에서 금지된 사랑을 경험하게 됩니다. 그러나 이 사랑은 파티의 규칙을 어기는 것으로 간주되며 위험한 상황에 처하게 됩니다.')]}\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder, FewShotChatMessagePromptTemplate\n",
    "\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# LLM 초기화\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "# 캐시 디렉토리 설정\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "\n",
    "# 텍스트 스플리터 설정\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "# URL에서 문서 로드\n",
    "loader = WebBaseLoader(\"https://gist.github.com/serranoarevalo/5acf755c2b8d83f1707ef266b82ea223\")\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "# cached_embeddings 설정\n",
    "embeddings = OpenAIEmbeddings()\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)\n",
    "\n",
    "# vectorstore 생성\n",
    "vectorstore = FAISS.from_documents(docs, cached_embeddings)\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# ConversationBufferMemory 초기화\n",
    "memory = ConversationBufferMemory(\n",
    "    return_messages=True,\n",
    "    memory_key=\"chat_history\"\n",
    ")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "def load_memory(_):\n",
    "    return memory.load_memory_variables({})[\"chat_history\"]\n",
    "\n",
    "# Stuff Documents prompt template\n",
    "stuff_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"당신은 제공된 컨텍스트를 기반으로 질문에 답하는 도움이 되는 어시스턴트입니다.\n",
    "    다음 컨텍스트 조각들을 사용하여 사용자의 질문에 답하세요.\n",
    "    컨텍스트를 기반으로 답을 모르는 경우, 모른다고 말하세요.\n",
    "    답을 지어내려고 하지 마세요.\n",
    "    \n",
    "    컨텍스트:\n",
    "    {context}\n",
    "    \"\"\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "# Stuff Documents Chain with memory\n",
    "def stuff_chain_with_memory(question):\n",
    "    # search relevant documents\n",
    "    docs = retriever.get_relevant_documents(question)\n",
    "    \n",
    "    # format docs\n",
    "    context = format_docs(docs)\n",
    "    \n",
    "    # load chat history\n",
    "    chat_history = load_memory(None)\n",
    "    \n",
    "    # create chain input\n",
    "    chain_input = {\n",
    "        \"context\": context,\n",
    "        \"chat_history\": chat_history,\n",
    "        \"question\": question\n",
    "    }\n",
    "    \n",
    "    # generate response\n",
    "    response = llm.invoke(stuff_prompt.format_messages(**chain_input))\n",
    "    \n",
    "    # save to memory\n",
    "    memory.save_context(\n",
    "        {\"input\": question},\n",
    "        {\"output\": response.content}\n",
    "    )\n",
    "    \n",
    "    return response\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": retriever | format_docs,\n",
    "        \"chat_history\": RunnableLambda(load_memory),\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | stuff_prompt\n",
    "    | llm\n",
    ")\n",
    "\n",
    "def invoke_chain_with_memory(question):\n",
    "    response = chain.invoke(question)\n",
    "    memory.save_context(\n",
    "        {\"input\": question},\n",
    "        {\"output\": response.content}\n",
    "    )\n",
    "    print(f\"Q: {question}\")\n",
    "    print(f\"A: {response.content}\")\n",
    "    print(\"-\" * 50)\n",
    "    return response\n",
    "\n",
    "# test RAG pipeline with questions\n",
    "print(\"=== Stuff Documents Chain RAG pipeline with memory ===\\n\")\n",
    "\n",
    "# clear memory\n",
    "memory.clear()\n",
    "\n",
    "# 질문 1: Aaronson이 유죄인가?\n",
    "print(\"1. Aaronson이 유죄인가?\")\n",
    "invoke_chain_with_memory(\"Aaronson은 유죄인가요?\")\n",
    "\n",
    "# 질문 2: 그가 테이블에 어떤 메시지를 썼나?\n",
    "print(\"2. 그가 테이블에 어떤 메시지를 썼나요?\")\n",
    "invoke_chain_with_memory(\"그가 테이블에 어떤 메시지를 썼나요?\")\n",
    "\n",
    "# 질문 3: Julia는 누구인가?\n",
    "print(\"3. Julia는 누구인가요?\")\n",
    "invoke_chain_with_memory(\"Julia는 누구인가요?\")\n",
    "\n",
    "# 메모리 내용 표시\n",
    "print(\"=== memory contents ===\")\n",
    "print(memory.load_memory_variables({}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
